{
  "cells": [
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "mOgTVvPnyb3l",
        "outputId": "7df4d1e9-c423-4ce5-c3c8-19716a9f216d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/drive\n"
          ]
        }
      ],
      "source": [
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "YW_iNlrDyG_l",
        "outputId": "662ee70d-b5d4-4e6e-ef80-c22d3cfcd3f5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting einops\n",
            "  Downloading einops-0.8.0-py3-none-any.whl (43 kB)\n",
            "\u001b[?25l     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m0.0/43.2 kB\u001b[0m \u001b[31m?\u001b[0m eta \u001b[36m-:--:--\u001b[0m\r\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m43.2/43.2 kB\u001b[0m \u001b[31m2.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: einops\n",
            "Successfully installed einops-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install einops"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OPz3lBvYyG_n"
      },
      "outputs": [],
      "source": [
        "import math\n",
        "import torch\n",
        "from torch import nn as nn\n",
        "from torch.nn import functional as F\n",
        "from torch.nn import init as init\n",
        "from torch.nn.modules.batchnorm import _BatchNorm\n",
        "\n",
        "# from basicsr.utils import get_root_logger\n",
        "\n",
        "# try:\n",
        "#     from basicsr.models.ops.dcn import (ModulatedDeformConvPack,\n",
        "#                                         modulated_deform_conv)\n",
        "# except ImportError:\n",
        "#     # print('Cannot import dcn. Ignore this warning if dcn is not used. '\n",
        "#     #       'Otherwise install BasicSR with compiling dcn.')\n",
        "#\n",
        "\n",
        "@torch.no_grad()\n",
        "def default_init_weights(module_list, scale=1, bias_fill=0, **kwargs):\n",
        "    \"\"\"Initialize network weights.\n",
        "\n",
        "    Args:\n",
        "        module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n",
        "        scale (float): Scale initialized weights, especially for residual\n",
        "            blocks. Default: 1.\n",
        "        bias_fill (float): The value to fill bias. Default: 0\n",
        "        kwargs (dict): Other arguments for initialization function.\n",
        "    \"\"\"\n",
        "    if not isinstance(module_list, list):\n",
        "        module_list = [module_list]\n",
        "    for module in module_list:\n",
        "        for m in module.modules():\n",
        "            if isinstance(m, nn.Conv2d):\n",
        "                init.kaiming_normal_(m.weight, **kwargs)\n",
        "                m.weight.data *= scale\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(bias_fill)\n",
        "            elif isinstance(m, nn.Linear):\n",
        "                init.kaiming_normal_(m.weight, **kwargs)\n",
        "                m.weight.data *= scale\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(bias_fill)\n",
        "            elif isinstance(m, _BatchNorm):\n",
        "                init.constant_(m.weight, 1)\n",
        "                if m.bias is not None:\n",
        "                    m.bias.data.fill_(bias_fill)\n",
        "\n",
        "\n",
        "def make_layer(basic_block, num_basic_block, **kwarg):\n",
        "    \"\"\"Make layers by stacking the same blocks.\n",
        "\n",
        "    Args:\n",
        "        basic_block (nn.module): nn.module class for basic block.\n",
        "        num_basic_block (int): number of blocks.\n",
        "\n",
        "    Returns:\n",
        "        nn.Sequential: Stacked blocks in nn.Sequential.\n",
        "    \"\"\"\n",
        "    layers = []\n",
        "    for _ in range(num_basic_block):\n",
        "        layers.append(basic_block(**kwarg))\n",
        "    return nn.Sequential(*layers)\n",
        "\n",
        "\n",
        "class ResidualBlockNoBN(nn.Module):\n",
        "    \"\"\"Residual block without BN.\n",
        "\n",
        "    It has a style of:\n",
        "        ---Conv-ReLU-Conv-+-\n",
        "         |________________|\n",
        "\n",
        "    Args:\n",
        "        num_feat (int): Channel number of intermediate features.\n",
        "            Default: 64.\n",
        "        res_scale (float): Residual scale. Default: 1.\n",
        "        pytorch_init (bool): If set to True, use pytorch default init,\n",
        "            otherwise, use default_init_weights. Default: False.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, num_feat=64, res_scale=1, pytorch_init=False):\n",
        "        super(ResidualBlockNoBN, self).__init__()\n",
        "        self.res_scale = res_scale\n",
        "        self.conv1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1, bias=True)\n",
        "        self.conv2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1, bias=True)\n",
        "        self.relu = nn.ReLU(inplace=True)\n",
        "\n",
        "        if not pytorch_init:\n",
        "            default_init_weights([self.conv1, self.conv2], 0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        identity = x\n",
        "        out = self.conv2(self.relu(self.conv1(x)))\n",
        "        return identity + out * self.res_scale\n",
        "\n",
        "\n",
        "class Upsample(nn.Sequential):\n",
        "    \"\"\"Upsample module.\n",
        "\n",
        "    Args:\n",
        "        scale (int): Scale factor. Supported scales: 2^n and 3.\n",
        "        num_feat (int): Channel number of intermediate features.\n",
        "    \"\"\"\n",
        "\n",
        "    def __init__(self, scale, num_feat):\n",
        "        m = []\n",
        "        if (scale & (scale - 1)) == 0:  # scale = 2^n\n",
        "            for _ in range(int(math.log(scale, 2))):\n",
        "                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n",
        "                m.append(nn.PixelShuffle(2))\n",
        "        elif scale == 3:\n",
        "            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n",
        "            m.append(nn.PixelShuffle(3))\n",
        "        else:\n",
        "            raise ValueError(f'scale {scale} is not supported. '\n",
        "                             'Supported scales: 2^n and 3.')\n",
        "        super(Upsample, self).__init__(*m)\n",
        "\n",
        "\n",
        "def flow_warp(x,\n",
        "              flow,\n",
        "              interp_mode='bilinear',\n",
        "              padding_mode='zeros',\n",
        "              align_corners=True):\n",
        "    \"\"\"Warp an image or feature map with optical flow.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Tensor with size (n, c, h, w).\n",
        "        flow (Tensor): Tensor with size (n, h, w, 2), normal value.\n",
        "        interp_mode (str): 'nearest' or 'bilinear'. Default: 'bilinear'.\n",
        "        padding_mode (str): 'zeros' or 'border' or 'reflection'.\n",
        "            Default: 'zeros'.\n",
        "        align_corners (bool): Before pytorch 1.3, the default value is\n",
        "            align_corners=True. After pytorch 1.3, the default value is\n",
        "            align_corners=False. Here, we use the True as default.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Warped image or feature map.\n",
        "    \"\"\"\n",
        "    assert x.size()[-2:] == flow.size()[1:3]\n",
        "    _, _, h, w = x.size()\n",
        "    # create mesh grid\n",
        "    grid_y, grid_x = torch.meshgrid(\n",
        "        torch.arange(0, h).type_as(x),\n",
        "        torch.arange(0, w).type_as(x))\n",
        "    grid = torch.stack((grid_x, grid_y), 2).float()  # W(x), H(y), 2\n",
        "    grid.requires_grad = False\n",
        "\n",
        "    vgrid = grid + flow\n",
        "    # scale grid to [-1,1]\n",
        "    vgrid_x = 2.0 * vgrid[:, :, :, 0] / max(w - 1, 1) - 1.0\n",
        "    vgrid_y = 2.0 * vgrid[:, :, :, 1] / max(h - 1, 1) - 1.0\n",
        "    vgrid_scaled = torch.stack((vgrid_x, vgrid_y), dim=3)\n",
        "    output = F.grid_sample(\n",
        "        x,\n",
        "        vgrid_scaled,\n",
        "        mode=interp_mode,\n",
        "        padding_mode=padding_mode,\n",
        "        align_corners=align_corners)\n",
        "\n",
        "    # TODO, what if align_corners=False\n",
        "    return output\n",
        "\n",
        "\n",
        "def resize_flow(flow,\n",
        "                size_type,\n",
        "                sizes,\n",
        "                interp_mode='bilinear',\n",
        "                align_corners=False):\n",
        "    \"\"\"Resize a flow according to ratio or shape.\n",
        "\n",
        "    Args:\n",
        "        flow (Tensor): Precomputed flow. shape [N, 2, H, W].\n",
        "        size_type (str): 'ratio' or 'shape'.\n",
        "        sizes (list[int | float]): the ratio for resizing or the final output\n",
        "            shape.\n",
        "            1) The order of ratio should be [ratio_h, ratio_w]. For\n",
        "            downsampling, the ratio should be smaller than 1.0 (i.e., ratio\n",
        "            < 1.0). For upsampling, the ratio should be larger than 1.0 (i.e.,\n",
        "            ratio > 1.0).\n",
        "            2) The order of output_size should be [out_h, out_w].\n",
        "        interp_mode (str): The mode of interpolation for resizing.\n",
        "            Default: 'bilinear'.\n",
        "        align_corners (bool): Whether align corners. Default: False.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: Resized flow.\n",
        "    \"\"\"\n",
        "    _, _, flow_h, flow_w = flow.size()\n",
        "    if size_type == 'ratio':\n",
        "        output_h, output_w = int(flow_h * sizes[0]), int(flow_w * sizes[1])\n",
        "    elif size_type == 'shape':\n",
        "        output_h, output_w = sizes[0], sizes[1]\n",
        "    else:\n",
        "        raise ValueError(\n",
        "            f'Size type should be ratio or shape, but got type {size_type}.')\n",
        "\n",
        "    input_flow = flow.clone()\n",
        "    ratio_h = output_h / flow_h\n",
        "    ratio_w = output_w / flow_w\n",
        "    input_flow[:, 0, :, :] *= ratio_w\n",
        "    input_flow[:, 1, :, :] *= ratio_h\n",
        "    resized_flow = F.interpolate(\n",
        "        input=input_flow,\n",
        "        size=(output_h, output_w),\n",
        "        mode=interp_mode,\n",
        "        align_corners=align_corners)\n",
        "    return resized_flow\n",
        "\n",
        "\n",
        "# TODO: may write a cpp file\n",
        "def pixel_unshuffle(x, scale):\n",
        "    \"\"\" Pixel unshuffle.\n",
        "\n",
        "    Args:\n",
        "        x (Tensor): Input feature with shape (b, c, hh, hw).\n",
        "        scale (int): Downsample ratio.\n",
        "\n",
        "    Returns:\n",
        "        Tensor: the pixel unshuffled feature.\n",
        "    \"\"\"\n",
        "    b, c, hh, hw = x.size()\n",
        "    out_channel = c * (scale**2)\n",
        "    assert hh % scale == 0 and hw % scale == 0\n",
        "    h = hh // scale\n",
        "    w = hw // scale\n",
        "    x_view = x.view(b, c, h, scale, w, scale)\n",
        "    return x_view.permute(0, 1, 3, 5, 2, 4).reshape(b, out_channel, h, w)\n",
        "\n",
        "\n",
        "# class DCNv2Pack(ModulatedDeformConvPack):\n",
        "#     \"\"\"Modulated deformable conv for deformable alignment.\n",
        "#\n",
        "#     Different from the official DCNv2Pack, which generates offsets and masks\n",
        "#     from the preceding features, this DCNv2Pack takes another different\n",
        "#     features to generate offsets and masks.\n",
        "#\n",
        "#     Ref:\n",
        "#         Delving Deep into Deformable Alignment in Video Super-Resolution.\n",
        "#     \"\"\"\n",
        "#\n",
        "#     def forward(self, x, feat):\n",
        "#         out = self.conv_offset(feat)\n",
        "#         o1, o2, mask = torch.chunk(out, 3, dim=1)\n",
        "#         offset = torch.cat((o1, o2), dim=1)\n",
        "#         mask = torch.sigmoid(mask)\n",
        "#\n",
        "#         offset_absmean = torch.mean(torch.abs(offset))\n",
        "#         if offset_absmean > 50:\n",
        "#             logger = get_root_logger()\n",
        "#             logger.warning(\n",
        "#                 f'Offset abs mean is {offset_absmean}, larger than 50.')\n",
        "#\n",
        "#         return modulated_deform_conv(x, offset, mask, self.weight, self.bias,\n",
        "#                                      self.stride, self.padding, self.dilation,\n",
        "#                                      self.groups, self.deformable_groups)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "EcMwDy6-yG_p"
      },
      "outputs": [],
      "source": [
        "## Restormer: Efficient Transformer for High-Resolution Image Restoration\n",
        "## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang\n",
        "## https://arxiv.org/abs/2111.09881\n",
        "\n",
        "\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from pdb import set_trace as stx\n",
        "import numbers\n",
        "\n",
        "from einops import rearrange\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "## Layer Norm\n",
        "\n",
        "def to_3d(x):\n",
        "    return rearrange(x, 'b c h w -> b (h w) c')\n",
        "\n",
        "def to_4d(x,h,w):\n",
        "    return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)\n",
        "\n",
        "class BiasFree_LayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape):\n",
        "        super(BiasFree_LayerNorm, self).__init__()\n",
        "        if isinstance(normalized_shape, numbers.Integral):\n",
        "            normalized_shape = (normalized_shape,)\n",
        "        normalized_shape = torch.Size(normalized_shape)\n",
        "\n",
        "        assert len(normalized_shape) == 1\n",
        "\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
        "        return x / torch.sqrt(sigma+1e-5) * self.weight\n",
        "\n",
        "class WithBias_LayerNorm(nn.Module):\n",
        "    def __init__(self, normalized_shape):\n",
        "        super(WithBias_LayerNorm, self).__init__()\n",
        "        if isinstance(normalized_shape, numbers.Integral):\n",
        "            normalized_shape = (normalized_shape,)\n",
        "        normalized_shape = torch.Size(normalized_shape)\n",
        "\n",
        "        assert len(normalized_shape) == 1\n",
        "\n",
        "        self.weight = nn.Parameter(torch.ones(normalized_shape))\n",
        "        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n",
        "        self.normalized_shape = normalized_shape\n",
        "\n",
        "    def forward(self, x):\n",
        "        mu = x.mean(-1, keepdim=True)\n",
        "        sigma = x.var(-1, keepdim=True, unbiased=False)\n",
        "        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias\n",
        "\n",
        "\n",
        "class LayerNorm(nn.Module):\n",
        "    def __init__(self, dim, LayerNorm_type):\n",
        "        super(LayerNorm, self).__init__()\n",
        "        if LayerNorm_type =='BiasFree':\n",
        "            self.body = BiasFree_LayerNorm(dim)\n",
        "        else:\n",
        "            self.body = WithBias_LayerNorm(dim)\n",
        "\n",
        "    def forward(self, x):\n",
        "        h, w = x.shape[-2:]\n",
        "        return to_4d(self.body(to_3d(x)), h, w)\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "## Gated-Dconv Feed-Forward Network (GDFN)\n",
        "class FeedForward(nn.Module):\n",
        "    def __init__(self, dim, ffn_expansion_factor, bias):\n",
        "        super(FeedForward, self).__init__()\n",
        "\n",
        "        hidden_features = int(dim*ffn_expansion_factor)\n",
        "\n",
        "        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n",
        "\n",
        "        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n",
        "\n",
        "        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.project_in(x)\n",
        "        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n",
        "        x = F.gelu(x1) * x2\n",
        "        x = self.project_out(x)\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "## Multi-DConv Head Transposed Self-Attention (MDTA)\n",
        "class Attention(nn.Module):\n",
        "    def __init__(self, dim, num_heads, bias):\n",
        "        super(Attention, self).__init__()\n",
        "        self.num_heads = num_heads\n",
        "        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n",
        "\n",
        "        self.qkv = nn.Conv2d(dim, dim*3, kernel_size=1, bias=bias)\n",
        "        self.qkv_dwconv = nn.Conv2d(dim*3, dim*3, kernel_size=3, stride=1, padding=1, groups=dim*3, bias=bias)\n",
        "        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n",
        "\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        b,c,h,w = x.shape\n",
        "\n",
        "        qkv = self.qkv_dwconv(self.qkv(x))\n",
        "        q,k,v = qkv.chunk(3, dim=1)\n",
        "\n",
        "        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
        "        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
        "        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n",
        "\n",
        "        q = torch.nn.functional.normalize(q, dim=-1)\n",
        "        k = torch.nn.functional.normalize(k, dim=-1)\n",
        "\n",
        "        attn = (q @ k.transpose(-2, -1)) * self.temperature\n",
        "        attn = attn.softmax(dim=-1)\n",
        "\n",
        "        out = (attn @ v)\n",
        "\n",
        "        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n",
        "\n",
        "        out = self.project_out(out)\n",
        "        return out\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "class TransformerBlock(nn.Module):\n",
        "    def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type):\n",
        "        super(TransformerBlock, self).__init__()\n",
        "\n",
        "        self.norm1 = LayerNorm(dim, LayerNorm_type)\n",
        "        self.attn = Attention(dim, num_heads, bias)\n",
        "        self.norm2 = LayerNorm(dim, LayerNorm_type)\n",
        "        self.ffn = FeedForward(dim, ffn_expansion_factor, bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = x + self.attn(self.norm1(x))\n",
        "        x = x + self.ffn(self.norm2(x))\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "## Overlapped image patch embedding with 3x3 Conv\n",
        "class OverlapPatchEmbed(nn.Module):\n",
        "    def __init__(self, in_c=3, embed_dim=48, bias=False):\n",
        "        super(OverlapPatchEmbed, self).__init__()\n",
        "\n",
        "        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.proj(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "\n",
        "\n",
        "##########################################################################\n",
        "## Resizing modules\n",
        "class Downsample(nn.Module):\n",
        "    def __init__(self, n_feat):\n",
        "        super(Downsample, self).__init__()\n",
        "\n",
        "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat//2, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                                  nn.PixelUnshuffle(2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)\n",
        "\n",
        "class Upsample(nn.Module):\n",
        "    def __init__(self, n_feat):\n",
        "        super(Upsample, self).__init__()\n",
        "\n",
        "        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat*2, kernel_size=3, stride=1, padding=1, bias=False),\n",
        "                                  nn.PixelShuffle(2))\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.body(x)\n",
        "\n",
        "##########################################################################\n",
        "##---------- Restormer -----------------------\n",
        "class RainEncoder(nn.Module):\n",
        "    def __init__(self,\n",
        "        inp_channels=3,\n",
        "        out_channels=3,\n",
        "        dim = 48,\n",
        "        num_blocks = [2,3,3,4],\n",
        "        num_refinement_blocks = 2,\n",
        "        heads = [1,2,4,8],\n",
        "        ffn_expansion_factor = 2.66,\n",
        "        bias = False,\n",
        "        LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n",
        "        dual_pixel_task = False        ## True for dual-pixel defocus deblurring only. Also set inp_channels=6\n",
        "    ):\n",
        "\n",
        "        super(RainEncoder, self).__init__()\n",
        "\n",
        "        self.GAP = nn.AdaptiveAvgPool2d(1)\n",
        "\n",
        "        self.patch_embed = OverlapPatchEmbed(inp_channels, dim)\n",
        "\n",
        "        self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
        "\n",
        "        self.down1_2 = Downsample(dim) ## From Level 1 to Level 2\n",
        "        self.encoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
        "\n",
        "        self.down2_3 = Downsample(int(dim*2**1)) ## From Level 2 to Level 3\n",
        "        self.encoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
        "\n",
        "        self.down3_4 = Downsample(int(dim*2**2)) ## From Level 3 to Level 4\n",
        "        self.latent = nn.Sequential(*[TransformerBlock(dim=int(dim*2**3), num_heads=heads[3], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[3])])\n",
        "        self.head = nn.Sequential(\n",
        "                        nn.Linear(384, 512),\n",
        "                        nn.ReLU(inplace=True),\n",
        "                        nn.Linear(512, 256)\n",
        "                    )\n",
        "        #self.weight_latent = nn.Linear(256, 384)\n",
        "\n",
        "\n",
        "    def forward(self, inp_img):\n",
        "\n",
        "        inp_enc_level1 = self.patch_embed(inp_img)\n",
        "        out_enc_level1 = self.encoder_level1(inp_enc_level1)\n",
        "\n",
        "        inp_enc_level2 = self.down1_2(out_enc_level1)\n",
        "        out_enc_level2 = self.encoder_level2(inp_enc_level2)\n",
        "\n",
        "        inp_enc_level3 = self.down2_3(out_enc_level2)\n",
        "        out_enc_level3 = self.encoder_level3(inp_enc_level3)\n",
        "\n",
        "        inp_enc_level4 = self.down3_4(out_enc_level3)\n",
        "        latent = self.latent(inp_enc_level4)\n",
        "        fea = self.GAP(latent)\n",
        "        fea = fea.reshape(fea.size(0), -1)\n",
        "        fea = self.head(fea)\n",
        "#         linear_weights = self.weight_latent(fea)\n",
        "#         print(fea.shape)\n",
        "\n",
        "        return latent, fea, out_enc_level1, out_enc_level2, out_enc_level3#, linear_weights"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8A--T6PDyG_q"
      },
      "outputs": [],
      "source": [
        "class RainDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "        inp_channels=3,\n",
        "        out_channels=3,\n",
        "        dim = 48,\n",
        "        num_blocks = [2,3,3,4],\n",
        "        num_refinement_blocks = 2,\n",
        "        heads = [1,2,4,8],\n",
        "        ffn_expansion_factor = 2.66,\n",
        "        bias = False,\n",
        "        LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n",
        "        dual_pixel_task = False        ## True for dual-pixel defocus deblurring only. Also set inp_channels=6\n",
        "    ):\n",
        "\n",
        "        super(RainDecoder, self).__init__()\n",
        "\n",
        "        self.up4_3 = Upsample(int(dim*2**3)) ## From Level 4 to Level 3\n",
        "#         self.reduce_chan_level3 = nn.Conv2d(int(dim*2**3), int(dim*2**2), kernel_size=1, bias=bias)\n",
        "        self.decoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
        "\n",
        "\n",
        "        self.up3_2 = Upsample(int(dim*2**2)) ## From Level 3 to Level 2\n",
        "#         self.reduce_chan_level2 = nn.Conv2d(int(dim*2**2), int(dim*2**1), kernel_size=1, bias=bias)\n",
        "        self.decoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
        "\n",
        "        self.up2_1 = Upsample(int(dim*2**1))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)\n",
        "\n",
        "        self.decoder_level1 = nn.Sequential(*[TransformerBlock(dim=int(dim**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
        "\n",
        "        self.output = nn.Conv2d(int(dim**1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
        "\n",
        "    def forward(self, latent):\n",
        "        inp_dec_level3 = self.up4_3(latent)\n",
        "        # inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)\n",
        "        # inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)\n",
        "        out_dec_level3 = self.decoder_level3(inp_dec_level3)\n",
        "\n",
        "        inp_dec_level2 = self.up3_2(out_dec_level3)\n",
        "        # inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)\n",
        "        # inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)\n",
        "        out_dec_level2 = self.decoder_level2(inp_dec_level2)\n",
        "\n",
        "        inp_dec_level1 = self.up2_1(out_dec_level2)\n",
        "        # inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)\n",
        "        out_dec_level1 = self.decoder_level1(inp_dec_level1)\n",
        "\n",
        "#         out_dec_level1 = self.refinement(out_dec_level1)\n",
        "\n",
        "        out_dec_level1 = self.output(out_dec_level1)\n",
        "\n",
        "\n",
        "        return out_dec_level1\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oGoypFUxyG_q"
      },
      "outputs": [],
      "source": [
        "class BackgroundDecoder(nn.Module):\n",
        "    def __init__(self,\n",
        "        inp_channels=3,\n",
        "        out_channels=3,\n",
        "        dim = 48,\n",
        "        num_blocks = [2,3,3,4],\n",
        "        num_refinement_blocks = 2,\n",
        "        heads = [1,2,4,8],\n",
        "        ffn_expansion_factor = 2.66,\n",
        "        bias = False,\n",
        "        LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n",
        "        dual_pixel_task = False        ## True for dual-pixel defocus deblurring only. Also set inp_channels=6\n",
        "    ):\n",
        "\n",
        "        super(BackgroundDecoder, self).__init__()\n",
        "\n",
        "        self.up4_3 = Upsample(int(dim*2**3)) ## From Level 4 to Level 3\n",
        "        self.reduce_chan_level3 = nn.Conv2d(int(dim*2**3), int(dim*2**2), kernel_size=1, bias=bias)\n",
        "        self.decoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n",
        "\n",
        "\n",
        "        self.up3_2 = Upsample(int(dim*2**2)) ## From Level 3 to Level 2\n",
        "        self.reduce_chan_level2 = nn.Conv2d(int(dim*2**2), int(dim*2**1), kernel_size=1, bias=bias)\n",
        "        self.decoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n",
        "\n",
        "        self.up2_1 = Upsample(int(dim*2**1))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)\n",
        "\n",
        "        self.decoder_level1 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n",
        "\n",
        "        self.refinement = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_refinement_blocks)])\n",
        "\n",
        "        #### For Dual-Pixel Defocus Deblurring Task ####\n",
        "        self.dual_pixel_task = dual_pixel_task\n",
        "        if self.dual_pixel_task:\n",
        "            self.skip_conv = nn.Conv2d(dim, int(dim*2**1), kernel_size=1, bias=bias)\n",
        "        ###########################\n",
        "\n",
        "        self.output = nn.Conv2d(int(dim*2**1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n",
        "\n",
        "    def forward(self, latent, out_enc_level1, out_enc_level2, out_enc_level3):\n",
        "        inp_dec_level3 = self.up4_3(latent)\n",
        "        inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)\n",
        "        inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)\n",
        "        out_dec_level3 = self.decoder_level3(inp_dec_level3)\n",
        "\n",
        "        inp_dec_level2 = self.up3_2(out_dec_level3)\n",
        "        inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)\n",
        "        inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)\n",
        "        out_dec_level2 = self.decoder_level2(inp_dec_level2)\n",
        "\n",
        "        inp_dec_level1 = self.up2_1(out_dec_level2)\n",
        "        inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)\n",
        "        out_dec_level1 = self.decoder_level1(inp_dec_level1)\n",
        "\n",
        "        out_dec_level1 = self.refinement(out_dec_level1)\n",
        "\n",
        "        out_dec_level1 = self.output(out_dec_level1)\n",
        "\n",
        "\n",
        "        return out_dec_level1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GCgxtKq8yG_q"
      },
      "outputs": [],
      "source": [
        "class DerainingFramework(nn.Module):\n",
        "    def __init__(self,\n",
        "        inp_channels=3,\n",
        "        out_channels=3,\n",
        "        is_train = False,\n",
        "        dim = 48,\n",
        "        num_blocks = [2,3,3,4],\n",
        "        num_refinement_blocks = 2,\n",
        "        heads = [1,2,4,8],\n",
        "        ffn_expansion_factor = 2.66,\n",
        "        bias = False,\n",
        "        LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n",
        "        dual_pixel_task = False        ## True for dual-pixel defocus deblurring only. Also set inp_channels=6\n",
        "    ):\n",
        "\n",
        "        super(DerainingFramework, self).__init__()\n",
        "        self.is_train = is_train\n",
        "        self.inp_rain_encoder = RainEncoder()\n",
        "        self.inp_rain_decoder = RainDecoder()\n",
        "        self.out_rain_encoder = RainEncoder()\n",
        "#         self.out_rain_decoder = RainDecoder()\n",
        "        self.inp_bg_encoder = RainEncoder()\n",
        "        self.inp_bg_decoder = BackgroundDecoder()\n",
        "        self.out_bg_encoder = RainEncoder()\n",
        "#         self.weight_latent = None\n",
        "#         self.out_bg_decoder = RainDecoder()\n",
        "\n",
        "\n",
        "    def forward(self, inp_img, weakly_img=[]):\n",
        "        if self.is_train:\n",
        "            latent_rain_weakly, weakly_rain_fea,_,_,_ = self.out_rain_encoder(weakly_img)\n",
        "            latent_bg_weakly, weakly_bg_fea,_,_,_ = self.out_bg_encoder(weakly_img)\n",
        "#             print(weight_latent.shape)\n",
        "#             self.weight_latent = weight_latent.view(-1,384,1,1)\n",
        "            latent_rain_inp, inp_rain_fea, rain_enc_level1, rain_enc_level2, rain_enc_level3 = self.inp_rain_encoder(inp_img)\n",
        "#             self.weight_latent = weight_latent.view(-1,384,1,1)\n",
        "#             latent_rain_inp = latent_rain_inp*self.weight_latent\n",
        "            rain_img = self.inp_rain_decoder(latent_rain_inp)\n",
        "            latent_bg, inp_bg_fea, bg_enc_level1, bg_enc_level2, bg_enc_level3 = self.inp_bg_encoder(inp_img)\n",
        "            latent_bg = latent_bg - latent_rain_inp\n",
        "            bg_img = self.inp_bg_decoder(latent_bg, bg_enc_level1, bg_enc_level2, bg_enc_level3)\n",
        "            re_rain_img = torch.clamp(rain_img + bg_img, min=0, max=1)\n",
        "\n",
        "            return inp_rain_fea, weakly_rain_fea, inp_bg_fea, weakly_bg_fea, re_rain_img, bg_img, rain_img\n",
        "        else:\n",
        "            latent_rain_inp, _,_,_,_ = self.inp_rain_encoder(inp_img)\n",
        "            rain_img = self.inp_rain_decoder(latent_rain_inp)\n",
        "            latent_bg, _, bg_enc_level1, bg_enc_level2, bg_enc_level3 = self.inp_bg_encoder(inp_img)\n",
        "#             latent_bg = latent_bg - latent_rain_inp\n",
        "            bg_img = self.inp_bg_decoder(latent_bg, bg_enc_level1, bg_enc_level2, bg_enc_level3)\n",
        "            return bg_img\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_Sh6_4YLyG_r"
      },
      "outputs": [],
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.optim as optim\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision import transforms\n",
        "from torchvision.datasets import DatasetFolder\n",
        "from torchvision.utils import save_image\n",
        "import torchvision\n",
        "\n",
        "import torch\n",
        "from torchvision import transforms\n",
        "from torch.utils.data import DataLoader, Dataset\n",
        "from torchvision.datasets import DatasetFolder, ImageFolder\n",
        "from torchvision.io import read_image\n",
        "import torchvision.transforms.functional as FT"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "A4pzYryFyG_s"
      },
      "outputs": [],
      "source": [
        "import random\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DOqgNcRHyG_t"
      },
      "outputs": [],
      "source": [
        "from tqdm import tqdm"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "dM9nYK8xyG_t"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MYVovh20yG_t",
        "outputId": "7478ae47-ac54-4786-a025-1d202b7cb9d5"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Using 1 GPUs!\n"
          ]
        }
      ],
      "source": [
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "model = DerainingFramework(is_train=False)\n",
        "# Wrap the model with DataParallel\n",
        "print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n",
        "model = nn.DataParallel(model)\n",
        "model.load_state_dict(torch.load('/content/drive/MyDrive/CS331/derain2024.pth'))\n",
        "\n",
        "model = model.to(device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZPveac6FyG_t"
      },
      "outputs": [],
      "source": [
        "# plt.imshow(dataset[0][0].resize((1280,720)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "OG7_0YTYyG_t",
        "outputId": "a068b043-8c50-4d38-b213-8bc4f3e24281"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Requirement already satisfied: natsort in /usr/local/lib/python3.10/dist-packages (8.4.0)\n"
          ]
        }
      ],
      "source": [
        "!pip install natsort"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "rplQWeDRyG_t"
      },
      "outputs": [],
      "source": [
        "from PIL import Image\n",
        "import os\n",
        "from natsort import natsorted\n",
        "from glob import glob\n",
        "import cv2\n",
        "# from skimage import\n",
        "import time\n",
        "from skimage import img_as_ubyte"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keBjQxglyG_t"
      },
      "outputs": [],
      "source": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h9xk2142yG_t"
      },
      "outputs": [],
      "source": [
        "# input_dir = '/kaggle/input/otherrealrain'\n",
        "# out_dir = '/kaggle/working/outputimg'\n",
        "# in_dir = '/kaggle/working/inputimg'\n",
        "# rain_dir = '/kaggle/working/rainmapimg'\n",
        "# os.makedirs(out_dir, exist_ok=True)\n",
        "# extensions = ['jpg', 'JPG', 'png', 'PNG', 'jpeg', 'JPEG', 'bmp', 'BMP']\n",
        "# files = natsorted(glob(os.path.join(input_dir, '*')))\n",
        "# print(files)\n",
        "\n",
        "# img_multiple_of = 8\n",
        "\n",
        "# # print(f\"\\n ==> Running {task} with weights {weights}\\n \")\n",
        "# with torch.no_grad():\n",
        "#   for filepath in tqdm(files):\n",
        "#       # print(file_)\n",
        "#       torch.cuda.ipc_collect()\n",
        "#       torch.cuda.empty_cache()\n",
        "#       img = cv2.cvtColor(cv2.imread(filepath), cv2.COLOR_BGR2RGB)\n",
        "#       img = cv2.resize(img, (640, 480))\n",
        "#       filename = os.path.split(filepath)[-1]\n",
        "#       cv2.imwrite(os.path.join(in_dir, filename),cv2.cvtColor(img, cv2.COLOR_RGB2BGR))\n",
        "#       input_ = torch.from_numpy(img).float().div(255.).permute(2,0,1).unsqueeze(0).cuda()\n",
        "\n",
        "#       # Pad the input if not_multiple_of 8\n",
        "#       h,w = input_.shape[2], input_.shape[3]\n",
        "#       H,W = ((h+img_multiple_of)//img_multiple_of)*img_multiple_of, ((w+img_multiple_of)//img_multiple_of)*img_multiple_of\n",
        "#       padh = H-h if h%img_multiple_of!=0 else 0\n",
        "#       padw = W-w if w%img_multiple_of!=0 else 0\n",
        "#       input_ = F.pad(input_, (0,padw,0,padh), 'reflect')\n",
        "# #       a = time.time()\n",
        "#       restored = model(input_)\n",
        "# #       b = time.time()\n",
        "# #       print(b-a)\n",
        "# #       break\n",
        "#       restored = torch.clamp(restored, 0, 1)\n",
        "\n",
        "#       # Unpad the output\n",
        "#       restored = restored[:,:,:h,:w]\n",
        "\n",
        "#       restored = restored.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
        "#       restored = img_as_ubyte(restored[0])\n",
        "\n",
        "#       input_ = input_[:,:,:h,:w]\n",
        "\n",
        "#       input_ = input_.permute(0, 2, 3, 1).cpu().detach().numpy()\n",
        "#       input_ = img_as_ubyte(input_[0])\n",
        "\n",
        "\n",
        "#       cv2.imwrite(os.path.join(out_dir, filename),cv2.cvtColor(restored, cv2.COLOR_RGB2BGR))\n",
        "#       plt.imshow(input_)\n",
        "#       plt.show()\n",
        "# #       cv2.imwrite(os.path.join(rain_dir, filename),cv2.cvtColor(input_ - restored, cv2.COLOR_RGB2GRAY))\n",
        "# #       plt.imshow(cv2.cvtColor(input_ - restored, cv2.COLOR_RGB2GRAY))\n",
        "# #       plt.show()\n",
        "#       plt.imshow(restored)\n",
        "#       plt.show()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LIwhYc4ryG_v",
        "outputId": "5ca205f6-bfa6-42ce-f443-b55bdf2a0364"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting streamlit\n",
            "  Downloading streamlit-1.35.0-py2.py3-none-any.whl (8.6 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m8.6/8.6 MB\u001b[0m \u001b[31m21.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: altair<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.2.2)\n",
            "Requirement already satisfied: blinker<2,>=1.0.0 in /usr/lib/python3/dist-packages (from streamlit) (1.4)\n",
            "Requirement already satisfied: cachetools<6,>=4.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (5.3.3)\n",
            "Requirement already satisfied: click<9,>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.1.7)\n",
            "Requirement already satisfied: numpy<2,>=1.19.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (1.25.2)\n",
            "Requirement already satisfied: packaging<25,>=16.8 in /usr/local/lib/python3.10/dist-packages (from streamlit) (24.0)\n",
            "Requirement already satisfied: pandas<3,>=1.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.0.3)\n",
            "Requirement already satisfied: pillow<11,>=7.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (9.4.0)\n",
            "Requirement already satisfied: protobuf<5,>=3.20 in /usr/local/lib/python3.10/dist-packages (from streamlit) (3.20.3)\n",
            "Requirement already satisfied: pyarrow>=7.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (14.0.2)\n",
            "Requirement already satisfied: requests<3,>=2.27 in /usr/local/lib/python3.10/dist-packages (from streamlit) (2.31.0)\n",
            "Requirement already satisfied: rich<14,>=10.14.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (13.7.1)\n",
            "Requirement already satisfied: tenacity<9,>=8.1.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (8.3.0)\n",
            "Requirement already satisfied: toml<2,>=0.10.1 in /usr/local/lib/python3.10/dist-packages (from streamlit) (0.10.2)\n",
            "Requirement already satisfied: typing-extensions<5,>=4.3.0 in /usr/local/lib/python3.10/dist-packages (from streamlit) (4.12.1)\n",
            "Collecting gitpython!=3.1.19,<4,>=3.0.7 (from streamlit)\n",
            "  Downloading GitPython-3.1.43-py3-none-any.whl (207 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.3/207.3 kB\u001b[0m \u001b[31m26.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting pydeck<1,>=0.8.0b4 (from streamlit)\n",
            "  Downloading pydeck-0.9.1-py2.py3-none-any.whl (6.9 MB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m6.9/6.9 MB\u001b[0m \u001b[31m60.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: tornado<7,>=6.0.3 in /usr/local/lib/python3.10/dist-packages (from streamlit) (6.3.3)\n",
            "Collecting watchdog>=2.1.5 (from streamlit)\n",
            "  Downloading watchdog-4.0.1-py3-none-manylinux2014_x86_64.whl (83 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m83.0/83.0 kB\u001b[0m \u001b[31m13.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: entrypoints in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.4)\n",
            "Requirement already satisfied: jinja2 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (3.1.4)\n",
            "Requirement already satisfied: jsonschema>=3.0 in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (4.19.2)\n",
            "Requirement already satisfied: toolz in /usr/local/lib/python3.10/dist-packages (from altair<6,>=4.0->streamlit) (0.12.1)\n",
            "Collecting gitdb<5,>=4.0.1 (from gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading gitdb-4.0.11-py3-none-any.whl (62 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 kB\u001b[0m \u001b[31m9.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hRequirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2.8.2)\n",
            "Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2023.4)\n",
            "Requirement already satisfied: tzdata>=2022.1 in /usr/local/lib/python3.10/dist-packages (from pandas<3,>=1.3.0->streamlit) (2024.1)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (3.7)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2.0.7)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2.27->streamlit) (2024.6.2)\n",
            "Requirement already satisfied: markdown-it-py>=2.2.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (3.0.0)\n",
            "Requirement already satisfied: pygments<3.0.0,>=2.13.0 in /usr/local/lib/python3.10/dist-packages (from rich<14,>=10.14.0->streamlit) (2.16.1)\n",
            "Collecting smmap<6,>=3.0.1 (from gitdb<5,>=4.0.1->gitpython!=3.1.19,<4,>=3.0.7->streamlit)\n",
            "  Downloading smmap-5.0.1-py3-none-any.whl (24 kB)\n",
            "Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.10/dist-packages (from jinja2->altair<6,>=4.0->streamlit) (2.1.5)\n",
            "Requirement already satisfied: attrs>=22.2.0 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (23.2.0)\n",
            "Requirement already satisfied: jsonschema-specifications>=2023.03.6 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (2023.12.1)\n",
            "Requirement already satisfied: referencing>=0.28.4 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.35.1)\n",
            "Requirement already satisfied: rpds-py>=0.7.1 in /usr/local/lib/python3.10/dist-packages (from jsonschema>=3.0->altair<6,>=4.0->streamlit) (0.18.1)\n",
            "Requirement already satisfied: mdurl~=0.1 in /usr/local/lib/python3.10/dist-packages (from markdown-it-py>=2.2.0->rich<14,>=10.14.0->streamlit) (0.1.2)\n",
            "Requirement already satisfied: six>=1.5 in /usr/local/lib/python3.10/dist-packages (from python-dateutil>=2.8.2->pandas<3,>=1.3.0->streamlit) (1.16.0)\n",
            "Installing collected packages: watchdog, smmap, pydeck, gitdb, gitpython, streamlit\n",
            "Successfully installed gitdb-4.0.11 gitpython-3.1.43 pydeck-0.9.1 smmap-5.0.1 streamlit-1.35.0 watchdog-4.0.1\n"
          ]
        }
      ],
      "source": [
        "!pip install streamlit"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "tHDDKMbOyG_v",
        "outputId": "b6aced0f-87c3-426c-933a-d234d9ea4bf0"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Writing my_app.py\n"
          ]
        }
      ],
      "source": [
        "%%writefile my_app.py\n",
        "\n",
        "import streamlit as st\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import cv2\n",
        "from PIL import Image\n",
        "import io\n",
        "\n",
        "def main():\n",
        "    st.write(\"\"\"#Deraining\"\"\")\n",
        "    img_multiple_of = 8\n",
        "\n",
        "    uploaded_file = st.file_uploader(\"Choose an image...\", type=[\"jpg\", \"jpeg\", \"png\"])\n",
        "    if uploaded_file is not None:\n",
        "        # Convert the file to an opencv image\n",
        "        file_bytes = np.asarray(bytearray(uploaded_file.read()), dtype=np.uint8)\n",
        "        image = cv2.imdecode(file_bytes, 1)\n",
        "        img = cv2.resize(image, (640, 480))\n",
        "        input_ = torch.from_numpy(img).float().div(255.).permute(2,0,1).unsqueeze(0).cuda()\n",
        "\n",
        "        # Pad the input if not_multiple_of 8\n",
        "        h,w = input_.shape[2], input_.shape[3]\n",
        "        H,W = ((h+img_multiple_of)//img_multiple_of)*img_multiple_of, ((w+img_multiple_of)//img_multiple_of)*img_multiple_of\n",
        "        padh = H-h if h%img_multiple_of!=0 else 0\n",
        "        padw = W-w if w%img_multiple_of!=0 else 0\n",
        "        input_ = F.pad(input_, (0,padw,0,padh), 'reflect')\n",
        "\n",
        "        restored = model(input_)\n",
        "\n",
        "        restored = torch.clamp(restored, 0, 1)\n",
        "\n",
        "        # Convert images to RGB\n",
        "        image_rgb = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n",
        "        deraining_image_rgb = cv2.cvtColor(restored, cv2.COLOR_BGR2RGB)\n",
        "\n",
        "        # Display images\n",
        "        st.write(\"### Original Image vs Deraining Image\")\n",
        "        col1, col2 = st.columns(2)\n",
        "        with col1:\n",
        "            st.image(image_rgb, caption='Original Image', use_column_width=True)\n",
        "        with col2:\n",
        "            st.image(deraining_image_rgb, caption='Deraining Image', use_column_width=True)\n",
        "\n",
        "if __name__ == '__main__':\n",
        "    main()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "kKeuGX0U1tEh",
        "outputId": "7d7154bd-6b0c-4f5d-8562-312e294ab453"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "34.125.22.33\n"
          ]
        }
      ],
      "source": [
        "! wget -q -O - ipv4.icanhazip.com"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wfo-DIaE1uhR",
        "outputId": "40538e49-994b-4b4c-d55a-22505a56dead"
      },
      "outputs": [
        {
          "metadata": {
            "tags": null
          },
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\n",
            "Collecting usage statistics. To deactivate, set browser.gatherUsageStats to false.\n",
            "\u001b[0m\n",
            "\u001b[K\u001b[?25hnpx: installed 22 in 4.353s\n",
            "your url is: https://tough-zebras-knock.loca.lt\n",
            "\u001b[0m\n",
            "\u001b[34m\u001b[1m  You can now view your Streamlit app in your browser.\u001b[0m\n",
            "\u001b[0m\n",
            "\u001b[34m  Local URL: \u001b[0m\u001b[1mhttp://localhost:8501\u001b[0m\n",
            "\u001b[34m  Network URL: \u001b[0m\u001b[1mhttp://172.28.0.12:8501\u001b[0m\n",
            "\u001b[34m  External URL: \u001b[0m\u001b[1mhttp://34.125.22.33:8501\u001b[0m\n",
            "\u001b[0m\n"
          ]
        }
      ],
      "source": [
        "! streamlit run my_app.py & npx localtunnel --port 8501"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kaggle": {
      "accelerator": "gpu",
      "dataSources": [
        {
          "datasetId": 3967758,
          "sourceId": 6908743,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4559025,
          "sourceId": 7793671,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4575715,
          "sourceId": 7812030,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4581682,
          "sourceId": 8125546,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4836464,
          "sourceId": 8171825,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4943448,
          "sourceId": 8321974,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4721781,
          "sourceId": 8352128,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4843440,
          "sourceId": 8367387,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4863452,
          "isSourceIdPinned": true,
          "sourceId": 8407320,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5033368,
          "sourceId": 8447008,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5062209,
          "sourceId": 8486049,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 3774994,
          "sourceId": 8487337,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4928100,
          "sourceId": 8487965,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5066634,
          "sourceId": 8492190,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5069845,
          "sourceId": 8496417,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 4916398,
          "sourceId": 8507162,
          "sourceType": "datasetVersion"
        },
        {
          "datasetId": 5036416,
          "sourceId": 8545051,
          "sourceType": "datasetVersion"
        }
      ],
      "dockerImageVersionId": 30664,
      "isGpuEnabled": true,
      "isInternetEnabled": true,
      "language": "python",
      "sourceType": "notebook"
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.13"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}