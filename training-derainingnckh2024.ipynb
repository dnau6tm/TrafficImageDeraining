{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.13","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"nvidiaTeslaT4","dataSources":[{"sourceId":6908743,"sourceType":"datasetVersion","datasetId":3967758},{"sourceId":7793671,"sourceType":"datasetVersion","datasetId":4559025},{"sourceId":7812030,"sourceType":"datasetVersion","datasetId":4575715},{"sourceId":8125546,"sourceType":"datasetVersion","datasetId":4581682},{"sourceId":8171825,"sourceType":"datasetVersion","datasetId":4836464},{"sourceId":8407320,"sourceType":"datasetVersion","datasetId":4863452,"isSourceIdPinned":true}],"dockerImageVersionId":30665,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"!pip install einops","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:53:44.066875Z","iopub.execute_input":"2024-05-23T16:53:44.067646Z","iopub.status.idle":"2024-05-23T16:53:58.719244Z","shell.execute_reply.started":"2024-05-23T16:53:44.067606Z","shell.execute_reply":"2024-05-23T16:53:58.718226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import math\nimport torch\nfrom torch import nn as nn\nfrom torch.nn import functional as F\nfrom torch.nn import init as init\nfrom torch.nn.modules.batchnorm import _BatchNorm\n\n# from basicsr.utils import get_root_logger\n\n# try:\n#     from basicsr.models.ops.dcn import (ModulatedDeformConvPack,\n#                                         modulated_deform_conv)\n# except ImportError:\n#     # print('Cannot import dcn. Ignore this warning if dcn is not used. '\n#     #       'Otherwise install BasicSR with compiling dcn.')\n#\n\n@torch.no_grad()\ndef default_init_weights(module_list, scale=1, bias_fill=0, **kwargs):\n    \"\"\"Initialize network weights.\n\n    Args:\n        module_list (list[nn.Module] | nn.Module): Modules to be initialized.\n        scale (float): Scale initialized weights, especially for residual\n            blocks. Default: 1.\n        bias_fill (float): The value to fill bias. Default: 0\n        kwargs (dict): Other arguments for initialization function.\n    \"\"\"\n    if not isinstance(module_list, list):\n        module_list = [module_list]\n    for module in module_list:\n        for m in module.modules():\n            if isinstance(m, nn.Conv2d):\n                init.kaiming_normal_(m.weight, **kwargs)\n                m.weight.data *= scale\n                if m.bias is not None:\n                    m.bias.data.fill_(bias_fill)\n            elif isinstance(m, nn.Linear):\n                init.kaiming_normal_(m.weight, **kwargs)\n                m.weight.data *= scale\n                if m.bias is not None:\n                    m.bias.data.fill_(bias_fill)\n            elif isinstance(m, _BatchNorm):\n                init.constant_(m.weight, 1)\n                if m.bias is not None:\n                    m.bias.data.fill_(bias_fill)\n\n\ndef make_layer(basic_block, num_basic_block, **kwarg):\n    \"\"\"Make layers by stacking the same blocks.\n\n    Args:\n        basic_block (nn.module): nn.module class for basic block.\n        num_basic_block (int): number of blocks.\n\n    Returns:\n        nn.Sequential: Stacked blocks in nn.Sequential.\n    \"\"\"\n    layers = []\n    for _ in range(num_basic_block):\n        layers.append(basic_block(**kwarg))\n    return nn.Sequential(*layers)\n\n\nclass ResidualBlockNoBN(nn.Module):\n    \"\"\"Residual block without BN.\n\n    It has a style of:\n        ---Conv-ReLU-Conv-+-\n         |________________|\n\n    Args:\n        num_feat (int): Channel number of intermediate features.\n            Default: 64.\n        res_scale (float): Residual scale. Default: 1.\n        pytorch_init (bool): If set to True, use pytorch default init,\n            otherwise, use default_init_weights. Default: False.\n    \"\"\"\n\n    def __init__(self, num_feat=64, res_scale=1, pytorch_init=False):\n        super(ResidualBlockNoBN, self).__init__()\n        self.res_scale = res_scale\n        self.conv1 = nn.Conv2d(num_feat, num_feat, 3, 1, 1, bias=True)\n        self.conv2 = nn.Conv2d(num_feat, num_feat, 3, 1, 1, bias=True)\n        self.relu = nn.ReLU(inplace=True)\n\n        if not pytorch_init:\n            default_init_weights([self.conv1, self.conv2], 0.1)\n\n    def forward(self, x):\n        identity = x\n        out = self.conv2(self.relu(self.conv1(x)))\n        return identity + out * self.res_scale\n\n\nclass Upsample(nn.Sequential):\n    \"\"\"Upsample module.\n\n    Args:\n        scale (int): Scale factor. Supported scales: 2^n and 3.\n        num_feat (int): Channel number of intermediate features.\n    \"\"\"\n\n    def __init__(self, scale, num_feat):\n        m = []\n        if (scale & (scale - 1)) == 0:  # scale = 2^n\n            for _ in range(int(math.log(scale, 2))):\n                m.append(nn.Conv2d(num_feat, 4 * num_feat, 3, 1, 1))\n                m.append(nn.PixelShuffle(2))\n        elif scale == 3:\n            m.append(nn.Conv2d(num_feat, 9 * num_feat, 3, 1, 1))\n            m.append(nn.PixelShuffle(3))\n        else:\n            raise ValueError(f'scale {scale} is not supported. '\n                             'Supported scales: 2^n and 3.')\n        super(Upsample, self).__init__(*m)\n\n\ndef flow_warp(x,\n              flow,\n              interp_mode='bilinear',\n              padding_mode='zeros',\n              align_corners=True):\n    \"\"\"Warp an image or feature map with optical flow.\n\n    Args:\n        x (Tensor): Tensor with size (n, c, h, w).\n        flow (Tensor): Tensor with size (n, h, w, 2), normal value.\n        interp_mode (str): 'nearest' or 'bilinear'. Default: 'bilinear'.\n        padding_mode (str): 'zeros' or 'border' or 'reflection'.\n            Default: 'zeros'.\n        align_corners (bool): Before pytorch 1.3, the default value is\n            align_corners=True. After pytorch 1.3, the default value is\n            align_corners=False. Here, we use the True as default.\n\n    Returns:\n        Tensor: Warped image or feature map.\n    \"\"\"\n    assert x.size()[-2:] == flow.size()[1:3]\n    _, _, h, w = x.size()\n    # create mesh grid\n    grid_y, grid_x = torch.meshgrid(\n        torch.arange(0, h).type_as(x),\n        torch.arange(0, w).type_as(x))\n    grid = torch.stack((grid_x, grid_y), 2).float()  # W(x), H(y), 2\n    grid.requires_grad = False\n\n    vgrid = grid + flow\n    # scale grid to [-1,1]\n    vgrid_x = 2.0 * vgrid[:, :, :, 0] / max(w - 1, 1) - 1.0\n    vgrid_y = 2.0 * vgrid[:, :, :, 1] / max(h - 1, 1) - 1.0\n    vgrid_scaled = torch.stack((vgrid_x, vgrid_y), dim=3)\n    output = F.grid_sample(\n        x,\n        vgrid_scaled,\n        mode=interp_mode,\n        padding_mode=padding_mode,\n        align_corners=align_corners)\n\n    # TODO, what if align_corners=False\n    return output\n\n\ndef resize_flow(flow,\n                size_type,\n                sizes,\n                interp_mode='bilinear',\n                align_corners=False):\n    \"\"\"Resize a flow according to ratio or shape.\n\n    Args:\n        flow (Tensor): Precomputed flow. shape [N, 2, H, W].\n        size_type (str): 'ratio' or 'shape'.\n        sizes (list[int | float]): the ratio for resizing or the final output\n            shape.\n            1) The order of ratio should be [ratio_h, ratio_w]. For\n            downsampling, the ratio should be smaller than 1.0 (i.e., ratio\n            < 1.0). For upsampling, the ratio should be larger than 1.0 (i.e.,\n            ratio > 1.0).\n            2) The order of output_size should be [out_h, out_w].\n        interp_mode (str): The mode of interpolation for resizing.\n            Default: 'bilinear'.\n        align_corners (bool): Whether align corners. Default: False.\n\n    Returns:\n        Tensor: Resized flow.\n    \"\"\"\n    _, _, flow_h, flow_w = flow.size()\n    if size_type == 'ratio':\n        output_h, output_w = int(flow_h * sizes[0]), int(flow_w * sizes[1])\n    elif size_type == 'shape':\n        output_h, output_w = sizes[0], sizes[1]\n    else:\n        raise ValueError(\n            f'Size type should be ratio or shape, but got type {size_type}.')\n\n    input_flow = flow.clone()\n    ratio_h = output_h / flow_h\n    ratio_w = output_w / flow_w\n    input_flow[:, 0, :, :] *= ratio_w\n    input_flow[:, 1, :, :] *= ratio_h\n    resized_flow = F.interpolate(\n        input=input_flow,\n        size=(output_h, output_w),\n        mode=interp_mode,\n        align_corners=align_corners)\n    return resized_flow\n\n\n# TODO: may write a cpp file\ndef pixel_unshuffle(x, scale):\n    \"\"\" Pixel unshuffle.\n\n    Args:\n        x (Tensor): Input feature with shape (b, c, hh, hw).\n        scale (int): Downsample ratio.\n\n    Returns:\n        Tensor: the pixel unshuffled feature.\n    \"\"\"\n    b, c, hh, hw = x.size()\n    out_channel = c * (scale**2)\n    assert hh % scale == 0 and hw % scale == 0\n    h = hh // scale\n    w = hw // scale\n    x_view = x.view(b, c, h, scale, w, scale)\n    return x_view.permute(0, 1, 3, 5, 2, 4).reshape(b, out_channel, h, w)\n\n\n# class DCNv2Pack(ModulatedDeformConvPack):\n#     \"\"\"Modulated deformable conv for deformable alignment.\n#\n#     Different from the official DCNv2Pack, which generates offsets and masks\n#     from the preceding features, this DCNv2Pack takes another different\n#     features to generate offsets and masks.\n#\n#     Ref:\n#         Delving Deep into Deformable Alignment in Video Super-Resolution.\n#     \"\"\"\n#\n#     def forward(self, x, feat):\n#         out = self.conv_offset(feat)\n#         o1, o2, mask = torch.chunk(out, 3, dim=1)\n#         offset = torch.cat((o1, o2), dim=1)\n#         mask = torch.sigmoid(mask)\n#\n#         offset_absmean = torch.mean(torch.abs(offset))\n#         if offset_absmean > 50:\n#             logger = get_root_logger()\n#             logger.warning(\n#                 f'Offset abs mean is {offset_absmean}, larger than 50.')\n#\n#         return modulated_deform_conv(x, offset, mask, self.weight, self.bias,\n#                                      self.stride, self.padding, self.dilation,\n#                                      self.groups, self.deformable_groups)\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:53:58.721771Z","iopub.execute_input":"2024-05-23T16:53:58.722251Z","iopub.status.idle":"2024-05-23T16:54:02.766590Z","shell.execute_reply.started":"2024-05-23T16:53:58.722217Z","shell.execute_reply":"2024-05-23T16:54:02.765719Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"## Restormer: Efficient Transformer for High-Resolution Image Restoration\n## Syed Waqas Zamir, Aditya Arora, Salman Khan, Munawar Hayat, Fahad Shahbaz Khan, and Ming-Hsuan Yang\n## https://arxiv.org/abs/2111.09881\n\n\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\nfrom pdb import set_trace as stx\nimport numbers\n\nfrom einops import rearrange\n\n\n\n##########################################################################\n## Layer Norm\n\ndef to_3d(x):\n    return rearrange(x, 'b c h w -> b (h w) c')\n\ndef to_4d(x,h,w):\n    return rearrange(x, 'b (h w) c -> b c h w',h=h,w=w)\n\nclass BiasFree_LayerNorm(nn.Module):\n    def __init__(self, normalized_shape):\n        super(BiasFree_LayerNorm, self).__init__()\n        if isinstance(normalized_shape, numbers.Integral):\n            normalized_shape = (normalized_shape,)\n        normalized_shape = torch.Size(normalized_shape)\n\n        assert len(normalized_shape) == 1\n\n        self.weight = nn.Parameter(torch.ones(normalized_shape))\n        self.normalized_shape = normalized_shape\n\n    def forward(self, x):\n        sigma = x.var(-1, keepdim=True, unbiased=False)\n        return x / torch.sqrt(sigma+1e-5) * self.weight\n\nclass WithBias_LayerNorm(nn.Module):\n    def __init__(self, normalized_shape):\n        super(WithBias_LayerNorm, self).__init__()\n        if isinstance(normalized_shape, numbers.Integral):\n            normalized_shape = (normalized_shape,)\n        normalized_shape = torch.Size(normalized_shape)\n\n        assert len(normalized_shape) == 1\n\n        self.weight = nn.Parameter(torch.ones(normalized_shape))\n        self.bias = nn.Parameter(torch.zeros(normalized_shape))\n        self.normalized_shape = normalized_shape\n\n    def forward(self, x):\n        mu = x.mean(-1, keepdim=True)\n        sigma = x.var(-1, keepdim=True, unbiased=False)\n        return (x - mu) / torch.sqrt(sigma+1e-5) * self.weight + self.bias\n\n\nclass LayerNorm(nn.Module):\n    def __init__(self, dim, LayerNorm_type):\n        super(LayerNorm, self).__init__()\n        if LayerNorm_type =='BiasFree':\n            self.body = BiasFree_LayerNorm(dim)\n        else:\n            self.body = WithBias_LayerNorm(dim)\n\n    def forward(self, x):\n        h, w = x.shape[-2:]\n        return to_4d(self.body(to_3d(x)), h, w)\n\n\n\n##########################################################################\n## Gated-Dconv Feed-Forward Network (GDFN)\nclass FeedForward(nn.Module):\n    def __init__(self, dim, ffn_expansion_factor, bias):\n        super(FeedForward, self).__init__()\n\n        hidden_features = int(dim*ffn_expansion_factor)\n\n        self.project_in = nn.Conv2d(dim, hidden_features*2, kernel_size=1, bias=bias)\n\n        self.dwconv = nn.Conv2d(hidden_features*2, hidden_features*2, kernel_size=3, stride=1, padding=1, groups=hidden_features*2, bias=bias)\n\n        self.project_out = nn.Conv2d(hidden_features, dim, kernel_size=1, bias=bias)\n\n    def forward(self, x):\n        x = self.project_in(x)\n        x1, x2 = self.dwconv(x).chunk(2, dim=1)\n        x = F.gelu(x1) * x2\n        x = self.project_out(x)\n        return x\n\n\n\n##########################################################################\n## Multi-DConv Head Transposed Self-Attention (MDTA)\nclass Attention(nn.Module):\n    def __init__(self, dim, num_heads, bias):\n        super(Attention, self).__init__()\n        self.num_heads = num_heads\n        self.temperature = nn.Parameter(torch.ones(num_heads, 1, 1))\n\n        self.qkv = nn.Conv2d(dim, dim*3, kernel_size=1, bias=bias)\n        self.qkv_dwconv = nn.Conv2d(dim*3, dim*3, kernel_size=3, stride=1, padding=1, groups=dim*3, bias=bias)\n        self.project_out = nn.Conv2d(dim, dim, kernel_size=1, bias=bias)\n\n\n\n    def forward(self, x):\n        b,c,h,w = x.shape\n\n        qkv = self.qkv_dwconv(self.qkv(x))\n        q,k,v = qkv.chunk(3, dim=1)\n\n        q = rearrange(q, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n        k = rearrange(k, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n        v = rearrange(v, 'b (head c) h w -> b head c (h w)', head=self.num_heads)\n\n        q = torch.nn.functional.normalize(q, dim=-1)\n        k = torch.nn.functional.normalize(k, dim=-1)\n\n        attn = (q @ k.transpose(-2, -1)) * self.temperature\n        attn = attn.softmax(dim=-1)\n\n        out = (attn @ v)\n\n        out = rearrange(out, 'b head c (h w) -> b (head c) h w', head=self.num_heads, h=h, w=w)\n\n        out = self.project_out(out)\n        return out\n\n\n\n##########################################################################\nclass TransformerBlock(nn.Module):\n    def __init__(self, dim, num_heads, ffn_expansion_factor, bias, LayerNorm_type):\n        super(TransformerBlock, self).__init__()\n\n        self.norm1 = LayerNorm(dim, LayerNorm_type)\n        self.attn = Attention(dim, num_heads, bias)\n        self.norm2 = LayerNorm(dim, LayerNorm_type)\n        self.ffn = FeedForward(dim, ffn_expansion_factor, bias)\n\n    def forward(self, x):\n        x = x + self.attn(self.norm1(x))\n        x = x + self.ffn(self.norm2(x))\n\n        return x\n\n\n\n##########################################################################\n## Overlapped image patch embedding with 3x3 Conv\nclass OverlapPatchEmbed(nn.Module):\n    def __init__(self, in_c=3, embed_dim=48, bias=False):\n        super(OverlapPatchEmbed, self).__init__()\n\n        self.proj = nn.Conv2d(in_c, embed_dim, kernel_size=3, stride=1, padding=1, bias=bias)\n\n    def forward(self, x):\n        x = self.proj(x)\n\n        return x\n\n\n\n##########################################################################\n## Resizing modules\nclass Downsample(nn.Module):\n    def __init__(self, n_feat):\n        super(Downsample, self).__init__()\n\n        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat//2, kernel_size=3, stride=1, padding=1, bias=False),\n                                  nn.PixelUnshuffle(2))\n\n    def forward(self, x):\n        return self.body(x)\n\nclass Upsample(nn.Module):\n    def __init__(self, n_feat):\n        super(Upsample, self).__init__()\n\n        self.body = nn.Sequential(nn.Conv2d(n_feat, n_feat*2, kernel_size=3, stride=1, padding=1, bias=False),\n                                  nn.PixelShuffle(2))\n\n    def forward(self, x):\n        return self.body(x)\n\n##########################################################################\n##---------- Restormer -----------------------\nclass RainEncoder(nn.Module):\n    def __init__(self,\n        inp_channels=3,\n        out_channels=3,\n        dim = 48,\n        num_blocks = [2,3,3,4],\n        num_refinement_blocks = 2,\n        heads = [1,2,4,8],\n        ffn_expansion_factor = 2.66,\n        bias = False,\n        LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n        dual_pixel_task = False        ## True for dual-pixel defocus deblurring only. Also set inp_channels=6\n    ):\n\n        super(RainEncoder, self).__init__()\n\n        self.GAP = nn.AdaptiveAvgPool2d(1)\n\n        self.patch_embed = OverlapPatchEmbed(inp_channels, dim)\n\n        self.encoder_level1 = nn.Sequential(*[TransformerBlock(dim=dim, num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n\n        self.down1_2 = Downsample(dim) ## From Level 1 to Level 2\n        self.encoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n\n        self.down2_3 = Downsample(int(dim*2**1)) ## From Level 2 to Level 3\n        self.encoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n\n        self.down3_4 = Downsample(int(dim*2**2)) ## From Level 3 to Level 4\n        self.latent = nn.Sequential(*[TransformerBlock(dim=int(dim*2**3), num_heads=heads[3], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[3])])\n        self.head = nn.Sequential(\n                        nn.Linear(384, 512),\n                        nn.ReLU(inplace=True),\n                        nn.Linear(512, 256)\n                    )\n        #self.weight_latent = nn.Linear(256, 384)\n        \n        \n    def forward(self, inp_img):\n\n        inp_enc_level1 = self.patch_embed(inp_img)\n        out_enc_level1 = self.encoder_level1(inp_enc_level1)\n\n        inp_enc_level2 = self.down1_2(out_enc_level1)\n        out_enc_level2 = self.encoder_level2(inp_enc_level2)\n\n        inp_enc_level3 = self.down2_3(out_enc_level2)\n        out_enc_level3 = self.encoder_level3(inp_enc_level3)\n\n        inp_enc_level4 = self.down3_4(out_enc_level3)\n        latent = self.latent(inp_enc_level4)\n        fea = self.GAP(latent)\n        fea = fea.reshape(fea.size(0), -1)\n        fea = self.head(fea)\n#         linear_weights = self.weight_latent(fea)\n#         print(fea.shape)\n\n        return latent, fea, out_enc_level1, out_enc_level2, out_enc_level3#, linear_weights","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:02.767990Z","iopub.execute_input":"2024-05-23T16:54:02.768449Z","iopub.status.idle":"2024-05-23T16:54:02.819309Z","shell.execute_reply.started":"2024-05-23T16:54:02.768420Z","shell.execute_reply":"2024-05-23T16:54:02.818425Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class RainDecoder(nn.Module):\n    def __init__(self,\n        inp_channels=3,\n        out_channels=3,\n        dim = 48,\n        num_blocks = [2,3,3,4],\n        num_refinement_blocks = 2,\n        heads = [1,2,4,8],\n        ffn_expansion_factor = 2.66,\n        bias = False,\n        LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n        dual_pixel_task = False        ## True for dual-pixel defocus deblurring only. Also set inp_channels=6\n    ):\n\n        super(RainDecoder, self).__init__()\n\n        self.up4_3 = Upsample(int(dim*2**3)) ## From Level 4 to Level 3\n#         self.reduce_chan_level3 = nn.Conv2d(int(dim*2**3), int(dim*2**2), kernel_size=1, bias=bias)\n        self.decoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n\n\n        self.up3_2 = Upsample(int(dim*2**2)) ## From Level 3 to Level 2\n#         self.reduce_chan_level2 = nn.Conv2d(int(dim*2**2), int(dim*2**1), kernel_size=1, bias=bias)\n        self.decoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n\n        self.up2_1 = Upsample(int(dim*2**1))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)\n\n        self.decoder_level1 = nn.Sequential(*[TransformerBlock(dim=int(dim**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n\n        self.output = nn.Conv2d(int(dim**1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n\n    def forward(self, latent):\n        inp_dec_level3 = self.up4_3(latent)\n        # inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)\n        # inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)\n        out_dec_level3 = self.decoder_level3(inp_dec_level3)\n\n        inp_dec_level2 = self.up3_2(out_dec_level3)\n        # inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)\n        # inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)\n        out_dec_level2 = self.decoder_level2(inp_dec_level2)\n\n        inp_dec_level1 = self.up2_1(out_dec_level2)\n        # inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)\n        out_dec_level1 = self.decoder_level1(inp_dec_level1)\n\n#         out_dec_level1 = self.refinement(out_dec_level1)\n\n        out_dec_level1 = self.output(out_dec_level1)\n\n        return out_dec_level1\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:02.823420Z","iopub.execute_input":"2024-05-23T16:54:02.823805Z","iopub.status.idle":"2024-05-23T16:54:02.837040Z","shell.execute_reply.started":"2024-05-23T16:54:02.823778Z","shell.execute_reply":"2024-05-23T16:54:02.836136Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class BackgroundDecoder(nn.Module):\n    def __init__(self,\n        inp_channels=3,\n        out_channels=3,\n        dim = 48,\n        num_blocks = [2,3,3,4],\n        num_refinement_blocks = 2,\n        heads = [1,2,4,8],\n        ffn_expansion_factor = 2.66,\n        bias = False,\n        LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n        dual_pixel_task = False        ## True for dual-pixel defocus deblurring only. Also set inp_channels=6\n    ):\n\n        super(BackgroundDecoder, self).__init__()\n\n        self.up4_3 = Upsample(int(dim*2**3)) ## From Level 4 to Level 3\n        self.reduce_chan_level3 = nn.Conv2d(int(dim*2**3), int(dim*2**2), kernel_size=1, bias=bias)\n        self.decoder_level3 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**2), num_heads=heads[2], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[2])])\n\n\n        self.up3_2 = Upsample(int(dim*2**2)) ## From Level 3 to Level 2\n        self.reduce_chan_level2 = nn.Conv2d(int(dim*2**2), int(dim*2**1), kernel_size=1, bias=bias)\n        self.decoder_level2 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[1], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[1])])\n\n        self.up2_1 = Upsample(int(dim*2**1))  ## From Level 2 to Level 1  (NO 1x1 conv to reduce channels)\n\n        self.decoder_level1 = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_blocks[0])])\n\n        self.refinement = nn.Sequential(*[TransformerBlock(dim=int(dim*2**1), num_heads=heads[0], ffn_expansion_factor=ffn_expansion_factor, bias=bias, LayerNorm_type=LayerNorm_type) for i in range(num_refinement_blocks)])\n\n        #### For Dual-Pixel Defocus Deblurring Task ####\n        self.dual_pixel_task = dual_pixel_task\n        if self.dual_pixel_task:\n            self.skip_conv = nn.Conv2d(dim, int(dim*2**1), kernel_size=1, bias=bias)\n        ###########################\n\n        self.output = nn.Conv2d(int(dim*2**1), out_channels, kernel_size=3, stride=1, padding=1, bias=bias)\n\n    def forward(self, latent, out_enc_level1, out_enc_level2, out_enc_level3):\n        inp_dec_level3 = self.up4_3(latent)\n        inp_dec_level3 = torch.cat([inp_dec_level3, out_enc_level3], 1)\n        inp_dec_level3 = self.reduce_chan_level3(inp_dec_level3)\n        out_dec_level3 = self.decoder_level3(inp_dec_level3)\n\n        inp_dec_level2 = self.up3_2(out_dec_level3)\n        inp_dec_level2 = torch.cat([inp_dec_level2, out_enc_level2], 1)\n        inp_dec_level2 = self.reduce_chan_level2(inp_dec_level2)\n        out_dec_level2 = self.decoder_level2(inp_dec_level2)\n\n        inp_dec_level1 = self.up2_1(out_dec_level2)\n        inp_dec_level1 = torch.cat([inp_dec_level1, out_enc_level1], 1)\n        out_dec_level1 = self.decoder_level1(inp_dec_level1)\n\n        out_dec_level1 = self.refinement(out_dec_level1)\n\n        out_dec_level1 = self.output(out_dec_level1)\n\n\n        return out_dec_level1","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:02.838305Z","iopub.execute_input":"2024-05-23T16:54:02.838681Z","iopub.status.idle":"2024-05-23T16:54:02.858468Z","shell.execute_reply.started":"2024-05-23T16:54:02.838649Z","shell.execute_reply":"2024-05-23T16:54:02.857384Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DerainingFramework(nn.Module):\n    def __init__(self,\n        inp_channels=3,\n        out_channels=3,\n        is_train = False,\n        dim = 48,\n        num_blocks = [2,3,3,4],\n        num_refinement_blocks = 2,\n        heads = [1,2,4,8],\n        ffn_expansion_factor = 2.66,\n        bias = False,\n        LayerNorm_type = 'WithBias',   ## Other option 'BiasFree'\n        dual_pixel_task = False        ## True for dual-pixel defocus deblurring only. Also set inp_channels=6\n    ):\n\n        super(DerainingFramework, self).__init__()\n        self.is_train = is_train\n        self.inp_rain_encoder = RainEncoder()\n        self.inp_rain_decoder = RainDecoder()\n        self.out_rain_encoder = RainEncoder()\n#         self.out_rain_decoder = RainDecoder()\n        self.inp_bg_encoder = RainEncoder()\n        self.inp_bg_decoder = BackgroundDecoder()\n        self.out_bg_encoder = RainEncoder()\n#         self.weight_latent = None\n#         self.out_bg_decoder = RainDecoder()\n\n\n    def forward(self, inp_img, weakly_img=[]):\n        if self.is_train:\n            latent_rain_weakly, weakly_rain_fea,_,_,_ = self.out_rain_encoder(weakly_img)\n            latent_bg_weakly, weakly_bg_fea,_,_,_ = self.out_bg_encoder(weakly_img)\n#             print(weight_latent.shape)\n#             self.weight_latent = weight_latent.view(-1,384,1,1)\n            latent_rain_inp, inp_rain_fea, rain_enc_level1, rain_enc_level2, rain_enc_level3 = self.inp_rain_encoder(inp_img)\n#             self.weight_latent = weight_latent.view(-1,384,1,1)\n#             latent_rain_inp = latent_rain_inp*self.weight_latent\n            rain_img = self.inp_rain_decoder(latent_rain_inp)\n            latent_bg, inp_bg_fea, bg_enc_level1, bg_enc_level2, bg_enc_level3 = self.inp_bg_encoder(inp_img)\n            latent_bg = latent_bg-latent_rain_inp\n            bg_img = self.inp_bg_decoder(latent_bg, bg_enc_level1, bg_enc_level2, bg_enc_level3)\n            re_rain_img = rain_img + bg_img\n\n            return inp_rain_fea, weakly_rain_fea, inp_bg_fea, weakly_bg_fea, re_rain_img, bg_img, rain_img\n        else:\n            latent_rain_inp, _,_,_,_,_ = self.inp_rain_encoder(inp_img)\n            rain_img = self.inp_rain_decoder(latent_rain_inp)\n            latent_bg, _, bg_enc_level1, bg_enc_level2, bg_enc_level3,_ = self.inp_bg_encoder(inp_img)\n            bg_img = self.inp_bg_decoder(latent_bg, bg_enc_level1, bg_enc_level2, bg_enc_level3)\n            return bg_img\n","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:02.860492Z","iopub.execute_input":"2024-05-23T16:54:02.860990Z","iopub.status.idle":"2024-05-23T16:54:02.879433Z","shell.execute_reply.started":"2024-05-23T16:54:02.860947Z","shell.execute_reply":"2024-05-23T16:54:02.877647Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def contrastiveloss(x1, x2, label, margin: float = 1.5):\n    \"\"\"\n    Computes Contrastive Loss\n    \"\"\"\n\n    dist = torch.nn.functional.pairwise_distance(x1, x2)\n\n    loss = (1 - label) * torch.pow(dist, 2) + (label) * torch.pow(torch.clamp(margin - dist, min=0.0), 2)\n    loss = torch.mean(loss)\n    \n    return loss","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:02.881876Z","iopub.execute_input":"2024-05-23T16:54:02.882730Z","iopub.status.idle":"2024-05-23T16:54:02.901746Z","shell.execute_reply.started":"2024-05-23T16:54:02.882657Z","shell.execute_reply":"2024-05-23T16:54:02.899667Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class ContrastiveLoss(nn.Module):\n    \"\"\"Contrastive loss.\n\n    Args:\n        loss_weight (float): Loss weight for contrastive loss. Default: 1.0.\n        reduction (str): Specifies the reduction to apply to the output.\n            Supported choices are 'none' | 'mean' | 'sum'. Default: 'mean'.\n    \"\"\"\n\n    def __init__(self, loss_weight=1.0, reduction='mean', label=0):\n        super(ContrastiveLoss, self).__init__()\n        if reduction not in ['none', 'mean', 'sum']:\n            raise ValueError(f'Unsupported reduction mode: {reduction}. '\n                             f'Supported ones are: {_reduction_modes}')\n\n        self.loss_weight = loss_weight\n        self.reduction = reduction\n        self.label = label\n\n    def forward(self, fea1, fea2, **kwargs):\n        \"\"\"\n        Args:\n\n        \"\"\"\n        return self.loss_weight * contrastiveloss(fea1, fea2, self.label)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:02.904356Z","iopub.execute_input":"2024-05-23T16:54:02.905116Z","iopub.status.idle":"2024-05-23T16:54:02.924405Z","shell.execute_reply.started":"2024-05-23T16:54:02.905055Z","shell.execute_reply":"2024-05-23T16:54:02.922700Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def l1_loss(pred, target):\n    return F.l1_loss(pred, target, reduction='none')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:02.926914Z","iopub.execute_input":"2024-05-23T16:54:02.927611Z","iopub.status.idle":"2024-05-23T16:54:02.941063Z","shell.execute_reply.started":"2024-05-23T16:54:02.927550Z","shell.execute_reply":"2024-05-23T16:54:02.939504Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class L1Loss(nn.Module):\n    \"\"\"L1 (mean absolute error, MAE) loss.\n\n    Args:\n        loss_weight (float): Loss weight for L1 loss. Default: 1.0.\n        reduction (str): Specifies the reduction to apply to the output.\n            Supported choices are 'none' | 'mean' | 'sum'. Default: 'mean'.\n    \"\"\"\n\n    def __init__(self, loss_weight=1.0, reduction='mean'):\n        super(L1Loss, self).__init__()\n        self.loss_weight = loss_weight\n        self.reduction = reduction\n\n    def forward(self, pred, target, weight=None, **kwargs):\n        \"\"\"\n        Args:\n            pred (Tensor): of shape (N, C, H, W). Predicted tensor.\n            target (Tensor): of shape (N, C, H, W). Ground truth tensor.\n            weight (Tensor, optional): of shape (N, C, H, W). Element-wise\n                weights. Default: None.\n        \"\"\"\n        return self.loss_weight * F.l1_loss(\n            pred, target, weight, reduction=self.reduction)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:02.947725Z","iopub.execute_input":"2024-05-23T16:54:02.949455Z","iopub.status.idle":"2024-05-23T16:54:02.962050Z","shell.execute_reply.started":"2024-05-23T16:54:02.949350Z","shell.execute_reply":"2024-05-23T16:54:02.960021Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def mse_loss(pred, target):\n    return F.mse_loss(pred, target, reduction='none')","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:02.964253Z","iopub.execute_input":"2024-05-23T16:54:02.964983Z","iopub.status.idle":"2024-05-23T16:54:02.984708Z","shell.execute_reply.started":"2024-05-23T16:54:02.964925Z","shell.execute_reply":"2024-05-23T16:54:02.983514Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class MSELoss(nn.Module):\n    \"\"\"L1 (mean absolute error, MAE) loss.\n\n    Args:\n        loss_weight (float): Loss weight for L2 loss. Default: 1.0.\n        reduction (str): Specifies the reduction to apply to the output.\n            Supported choices are 'none' | 'mean' | 'sum'. Default: 'mean'.\n    \"\"\"\n\n    def __init__(self, loss_weight=1.0, reduction='mean'):\n        super(MSELoss, self).__init__()\n        self.loss_weight = loss_weight\n        self.reduction = reduction\n\n    def forward(self, pred, target, weight=None, **kwargs):\n        \"\"\"\n        Args:\n            pred (Tensor): of shape (N, C, H, W). Predicted tensor.\n            target (Tensor): of shape (N, C, H, W). Ground truth tensor.\n            weight (Tensor, optional): of shape (N, C, H, W). Element-wise\n                weights. Default: None.\n        \"\"\"\n        return self.loss_weight * F.mse_loss(\n            pred, target, weight, reduction=self.reduction)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:02.986614Z","iopub.execute_input":"2024-05-23T16:54:02.987045Z","iopub.status.idle":"2024-05-23T16:54:03.004447Z","shell.execute_reply.started":"2024-05-23T16:54:02.987008Z","shell.execute_reply":"2024-05-23T16:54:03.002401Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# class SparsityLoss(nn.Module):\n#     def __init__(self, loss_weight=1.0, reduction='mean'):\n#         super(SparsityLoss, self).__init__()\n#         self.loss_weight = loss_weight\n#         self.reduction = reduction\n\n#     def forward(self, pred, weight=None, **kwargs):\n#         return self.loss_weight * (torch.norm(pred, p=1, dim=(2, 3)).sum(dim=1))# + 0.2*(256*256*3-torch.count_nonzero(torch.clamp(pred-0.1, min=0, max=1), dim=(2,3)).sum(dim=1)))","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:03.006354Z","iopub.execute_input":"2024-05-23T16:54:03.006718Z","iopub.status.idle":"2024-05-23T16:54:03.020327Z","shell.execute_reply.started":"2024-05-23T16:54:03.006693Z","shell.execute_reply":"2024-05-23T16:54:03.018322Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nimport torch.optim as optim\nfrom torch.utils.data import DataLoader\nfrom torchvision import transforms\nfrom torchvision.datasets import DatasetFolder\nfrom torchvision.utils import save_image\nimport torchvision\n\nimport torch\nfrom torchvision import transforms\nfrom torch.utils.data import DataLoader, Dataset\nfrom torchvision.datasets import DatasetFolder, ImageFolder\nfrom torchvision.io import read_image\nimport torchvision.transforms.functional as FT","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:03.022188Z","iopub.execute_input":"2024-05-23T16:54:03.022537Z","iopub.status.idle":"2024-05-23T16:54:06.401644Z","shell.execute_reply.started":"2024-05-23T16:54:03.022514Z","shell.execute_reply":"2024-05-23T16:54:06.400858Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"image_size = 256\n\n# Đường dẫn đến thư mục chứa ảnh input và ground truth\nroot_dataset = '/kaggle/input/downscaleraindataset/RealTrafficRain/train/TrafficRain'\n\ndataset = ImageFolder(root=root_dataset)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:06.402821Z","iopub.execute_input":"2024-05-23T16:54:06.403178Z","iopub.status.idle":"2024-05-23T16:54:11.480238Z","shell.execute_reply.started":"2024-05-23T16:54:06.403154Z","shell.execute_reply":"2024-05-23T16:54:11.479342Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dataset[6800][1]","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.481319Z","iopub.execute_input":"2024-05-23T16:54:11.481643Z","iopub.status.idle":"2024-05-23T16:54:11.525113Z","shell.execute_reply.started":"2024-05-23T16:54:11.481609Z","shell.execute_reply":"2024-05-23T16:54:11.524175Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import random\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.527151Z","iopub.execute_input":"2024-05-23T16:54:11.527877Z","iopub.status.idle":"2024-05-23T16:54:11.532016Z","shell.execute_reply.started":"2024-05-23T16:54:11.527842Z","shell.execute_reply":"2024-05-23T16:54:11.530977Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class DerainingDataset(torch.utils.data.Dataset):\n    def __init__(self, dataset):\n        self.dataset = dataset\n        \n    def transform(self, image, mask):\n        # Resize\n        resize = transforms.Resize(size=(640, 480))\n        image = resize(image)\n        mask = resize(mask)\n\n        # Random crop\n        seed = np.random.randint(42)\n        random.seed(seed)\n        torch.manual_seed(seed)\n        i, j, h, w = transforms.RandomCrop.get_params(\n            image, output_size=(256, 256))\n        image = FT.crop(image, i, j, h, w)\n        mask = FT.crop(mask, i, j, h, w)\n        \n        image = FT.to_tensor(image)\n        mask = FT.to_tensor(mask)\n        return image, mask\n        \n    def __getitem__(self, index):\n        input_image = self.dataset[index%6800][0]\n        ground_truth_image = self.dataset[6800+index%6800][0]\n        return self.transform(input_image, ground_truth_image)\n\n    def __len__(self):\n        return 6800\n\n# Tạo đối tượng dataset deraining\nrandom.seed(42)\nderaining_dataset = DerainingDataset(dataset)\n\n# Tạo DataLoader cho việc training\nbatch_size = 2\nderaining_dataloader = DataLoader(deraining_dataset, batch_size=batch_size, shuffle=True, num_workers=4)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.533233Z","iopub.execute_input":"2024-05-23T16:54:11.533579Z","iopub.status.idle":"2024-05-23T16:54:11.545592Z","shell.execute_reply.started":"2024-05-23T16:54:11.533545Z","shell.execute_reply":"2024-05-23T16:54:11.544710Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(f\"Number of training examples: {len(deraining_dataset)}\")\nprint(f\"Number of batches in the dataloader: {len(deraining_dataloader)}\")","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.546691Z","iopub.execute_input":"2024-05-23T16:54:11.546981Z","iopub.status.idle":"2024-05-23T16:54:11.560010Z","shell.execute_reply.started":"2024-05-23T16:54:11.546956Z","shell.execute_reply":"2024-05-23T16:54:11.559164Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from tqdm import tqdm","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.561222Z","iopub.execute_input":"2024-05-23T16:54:11.562101Z","iopub.status.idle":"2024-05-23T16:54:11.569896Z","shell.execute_reply.started":"2024-05-23T16:54:11.562066Z","shell.execute_reply":"2024-05-23T16:54:11.568917Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import matplotlib.pyplot as plt\nimport numpy as np","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.571219Z","iopub.execute_input":"2024-05-23T16:54:11.571547Z","iopub.status.idle":"2024-05-23T16:54:11.578788Z","shell.execute_reply.started":"2024-05-23T16:54:11.571513Z","shell.execute_reply":"2024-05-23T16:54:11.577815Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n# model = DerainingFramework(is_train=False)\n# # Wrap the model with DataParallel\n# if torch.cuda.device_count() > 1:\n#     print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n#     model = nn.DataParallel(model, device_ids=[0, 1])\n# model.load_state_dict(torch.load('/kaggle/input/2004verlightblue/ver1804_derain2024_7.pth'))\n\n# model = model.to(device)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.580125Z","iopub.execute_input":"2024-05-23T16:54:11.580529Z","iopub.status.idle":"2024-05-23T16:54:11.589163Z","shell.execute_reply.started":"2024-05-23T16:54:11.580497Z","shell.execute_reply":"2024-05-23T16:54:11.588458Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# plt.imshow(dataset[0][0].resize((1280,720)))","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.590406Z","iopub.execute_input":"2024-05-23T16:54:11.590679Z","iopub.status.idle":"2024-05-23T16:54:11.600091Z","shell.execute_reply.started":"2024-05-23T16:54:11.590655Z","shell.execute_reply":"2024-05-23T16:54:11.599226Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# !mkdir /kaggle/working/outputimg\n# !mkdir /kaggle/working/rainmapimg\n# !mkdir /kaggle/working/inputimg","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.601099Z","iopub.execute_input":"2024-05-23T16:54:11.601403Z","iopub.status.idle":"2024-05-23T16:54:11.609142Z","shell.execute_reply.started":"2024-05-23T16:54:11.601355Z","shell.execute_reply":"2024-05-23T16:54:11.608416Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from PIL import Image","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.610380Z","iopub.execute_input":"2024-05-23T16:54:11.610676Z","iopub.status.idle":"2024-05-23T16:54:11.622911Z","shell.execute_reply.started":"2024-05-23T16:54:11.610653Z","shell.execute_reply":"2024-05-23T16:54:11.621856Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"topil = transforms.ToPILImage()\n# idclear = 0\n# idrain = 0\n# idinput = 0\n# for i, (input_images, weakly_grths) in enumerate(tqdm(deraining_dataloader)):\n#         input_images = input_images.to(device)\n#         weakly_grths = weakly_grths.to(device)\n        \n#         clear_imgs = model(input_images)\n#         clear_imgs = clear_imgs\n# #         print(clear_imgs.shape)\n#         for clear_img in clear_imgs:\n#             print(torch.min(clear_img))\n#             print(torch.max(clear_img))\n#             print(torch.count_nonzero(clear_img))\n#             pil_clear_img = topil(clear_img.detach().cpu())\n#             pil_clear_img.save(f'/kaggle/working/outputimg/{idclear}.png')\n#             img = Image.open(f'/kaggle/working/outputimg/{idclear}.png')\n#             plt.imshow(img)\n#             plt.show()\n#             idclear += 1\n# #         for rainmap_img in clear_imgs:\n# #             pil_rainmap_img = topil(rainmap_img)\n# #             pil_rainmap_img.save(f'/kaggle/working/rainmapimg/{idrain}.png')\n# #             idrain += 1\n#         for input_img in input_images:\n#             pil_input_img = topil(input_img)\n#             pil_input_img.save(f'/kaggle/working/inputimg/{idinput}.png')\n# #             img2 = Image.open(f'/kaggle/working/outputimg/{idinput}.png')\n# #             plt.imshow(img2)\n# #             plt.show()\n#             idinput += 1\n            \n#         if idinput == 30:\n#             break\n            ","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.626092Z","iopub.execute_input":"2024-05-23T16:54:11.626377Z","iopub.status.idle":"2024-05-23T16:54:11.633354Z","shell.execute_reply.started":"2024-05-23T16:54:11.626329Z","shell.execute_reply":"2024-05-23T16:54:11.632507Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# clear_img[0][0][:3,:128,:128].shape","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.634649Z","iopub.execute_input":"2024-05-23T16:54:11.634929Z","iopub.status.idle":"2024-05-23T16:54:11.643931Z","shell.execute_reply.started":"2024-05-23T16:54:11.634906Z","shell.execute_reply":"2024-05-23T16:54:11.642931Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i, (input_images, weakly_grths) in enumerate(tqdm(deraining_dataloader)):\n#         input_images = input_images.to(device)\n#         weakly_grths = weakly_grths.to(device)\n#         print(torch.norm(input_images, p=1, dim=(2, 3)).sum(dim=1))\n#         print(torch.count_nonzero(input_images, dim=(2,3)).sum(dim=1))","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.649524Z","iopub.execute_input":"2024-05-23T16:54:11.649785Z","iopub.status.idle":"2024-05-23T16:54:11.654676Z","shell.execute_reply.started":"2024-05-23T16:54:11.649763Z","shell.execute_reply":"2024-05-23T16:54:11.653593Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def split_image_into_patches(image_tensor, patch_size, stride):\n    \"\"\"\n    Split an image tensor into patches.\n    \n    Args:\n        image_tensor (torch.Tensor): Input image tensor of shape (N, C, H, W).\n        patch_size (int or tuple): Size of the patches, can be a single integer or a tuple (patch_height, patch_width).\n        stride (int or tuple): Stride of the sliding window, can be a single integer or a tuple (vertical_stride, horizontal_stride).\n        \n    Returns:\n        torch.Tensor: Tensor containing patches of shape (N, num_patches, C, patch_height, patch_width).\n    \"\"\"\n    # If patch_size or stride is an integer, convert to tuple\n    if isinstance(patch_size, int):\n        patch_size = (patch_size, patch_size)\n    if isinstance(stride, int):\n        stride = (stride, stride)\n    \n    # Unfold the image tensor to get patches\n    patches = image_tensor.unfold(2,patch_size[0], stride[0]).unfold(3,patch_size[0], stride[0]).permute(0,2,3,1,4,5).reshape(2,-1,3,patch_size[0], patch_size[0])\n    \n    return patches","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.656002Z","iopub.execute_input":"2024-05-23T16:54:11.656794Z","iopub.status.idle":"2024-05-23T16:54:11.664811Z","shell.execute_reply.started":"2024-05-23T16:54:11.656767Z","shell.execute_reply":"2024-05-23T16:54:11.663777Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# for i, (input_images, weakly_grths) in enumerate(tqdm(deraining_dataloader)):\n#     print(input_images[0].shape)\n#     pil_inp = topil(input_images[0])\n#     plt.imshow(pil_inp)\n#     plt.show()\n#     patch_outputs = split_image_into_patches(input_images, (64,64),(32,32)).detach().cpu()\n#     print(patch_outputs.shape)\n#     for img1, img2 in zip(patch_outputs[0], patch_outputs[1]):\n#         pil_img1 = topil(img1)\n#         pil_img2 = topil(img2)\n#         plt.imshow(pil_img1)\n#         plt.show()\n\n#         plt.imshow(pil_img2)\n#         plt.show()\n#         break\n\n\n\n#     break","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.665951Z","iopub.execute_input":"2024-05-23T16:54:11.666237Z","iopub.status.idle":"2024-05-23T16:54:11.678135Z","shell.execute_reply.started":"2024-05-23T16:54:11.666215Z","shell.execute_reply":"2024-05-23T16:54:11.677080Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"import torch\n\ndef compute_l2_distance_per_patch_pair(patch_tensor1, patch_tensor2):\n    \"\"\"\n    Compute L2 distance for each pair of patches between two sets of patch tensors.\n    \n    Args:\n        patch_tensor1 (torch.Tensor): First set of patch tensors of shape (N, num_patches, C, patch_height, patch_width).\n        patch_tensor2 (torch.Tensor): Second set of patch tensors of shape (N, num_patches, C, patch_height, patch_width).\n        \n    Returns:\n        torch.Tensor: L2 distances for each pair of patches of shape (N, num_patches).\n    \"\"\"\n    # Reshape tensors for broadcasting\n    patch_tensor1_expanded = patch_tensor1.unsqueeze(2)  # Shape: (N, num_patches, 1, C, patch_height, patch_width)\n    patch_tensor2_expanded = patch_tensor2.unsqueeze(2)  # Shape: (N, num_patches, 1, C, patch_height, patch_width)\n    \n    # Compute element-wise squared difference\n    squared_diff = (patch_tensor1_expanded - patch_tensor2_expanded) ** 2\n    \n    # Sum along the channel, height, and width axes\n    sum_squared_diff = torch.sum(squared_diff, dim=(2, 3, 4, 5))\n    \n    # Compute square root to get L2 distance\n    l2_distance = torch.sqrt(sum_squared_diff)\n    \n    return l2_distance\n\n# Example usage\npatch_tensor1 = torch.randn(1, 49, 3, 64, 64)  # First set of patch tensors\npatch_tensor2 = torch.randn(1, 49, 3, 64, 64)  # Second set of patch tensors\nl2_distances = compute_l2_distance_per_patch_pair(patch_tensor1, patch_tensor2)\n\nprint(\"L2 distances shape:\", l2_distances[:,:].shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.679338Z","iopub.execute_input":"2024-05-23T16:54:11.680177Z","iopub.status.idle":"2024-05-23T16:54:11.821811Z","shell.execute_reply.started":"2024-05-23T16:54:11.680151Z","shell.execute_reply":"2024-05-23T16:54:11.820873Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# l2 = l2_distances[:,:]\n\n# indices = torch.topk(l2, dim=1, k=5).indices","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.822949Z","iopub.execute_input":"2024-05-23T16:54:11.823250Z","iopub.status.idle":"2024-05-23T16:54:11.827303Z","shell.execute_reply.started":"2024-05-23T16:54:11.823226Z","shell.execute_reply":"2024-05-23T16:54:11.826428Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# indices","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.828520Z","iopub.execute_input":"2024-05-23T16:54:11.829216Z","iopub.status.idle":"2024-05-23T16:54:11.837815Z","shell.execute_reply.started":"2024-05-23T16:54:11.829189Z","shell.execute_reply":"2024-05-23T16:54:11.836936Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# patch_tensor1[torch.arange(patch_tensor1.size(0)).unsqueeze(1), indices].shape","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.838921Z","iopub.execute_input":"2024-05-23T16:54:11.839212Z","iopub.status.idle":"2024-05-23T16:54:11.848313Z","shell.execute_reply.started":"2024-05-23T16:54:11.839188Z","shell.execute_reply":"2024-05-23T16:54:11.847178Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# print(patch_tensor1[0][indices[0]].shape)","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.849667Z","iopub.execute_input":"2024-05-23T16:54:11.850483Z","iopub.status.idle":"2024-05-23T16:54:11.857917Z","shell.execute_reply.started":"2024-05-23T16:54:11.850449Z","shell.execute_reply":"2024-05-23T16:54:11.857157Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\nmodel = DerainingFramework(is_train=True)\n\n# Wrap the model with DataParallel\nif torch.cuda.device_count() > 1:\n    print(\"Using\", torch.cuda.device_count(), \"GPUs!\")\n    model = nn.DataParallel(model, device_ids=[0, 1])\n    \nmodel.load_state_dict(torch.load('/kaggle/input/2304verocean/wo_spar_ver1205_derain2024_7.pth'))\n\nmodel = model.to(device)\n\nlearning_rate = 1e-5\n# Define the loss function and optimizers\nsparsity_loss = L1Loss(loss_weight=0.1)\nmse_loss = MSELoss()\nw_l1_loss = MSELoss(loss_weight=0.1)\ncl_pos = ContrastiveLoss(label=0, loss_weight=1)\ncl_neg = ContrastiveLoss(label=1, loss_weight=1)\noptimizer = optim.Adam(model.parameters(), lr=learning_rate)\n\n# Training loop\nnum_epochs = 5\n\nfor epoch in range(num_epochs):\n    training_loss = 0.0\n    rain_loss = 0.0\n    background_loss = 0.0\n    con_loss = 0.0\n    w_con_loss = 0.0\n    spar_loss = 0.0\n    for i, (input_images, weakly_grths) in enumerate(tqdm(deraining_dataloader)):\n        input_images = input_images.to(device)\n#         cp_weakly_grths = weakly_grths\n        weakly_grths = weakly_grths.to(device)\n        \n        optimizer.zero_grad()\n\n        inp_rain_fea, weakly_rain_fea, inp_bg_fea, weakly_bg_fea, re_rain_img, bg_img, rain_img = model(input_images, weakly_grths)\n\n#         patch_outputs = split_image_into_patches(bg_img, (64, 64), (32, 32)).detach().cpu()\n#         patch_inputs = split_image_into_patches(input_images, (64, 64), (32, 32)).detach().cpu()\n#         patch_grths = split_image_into_patches(weakly_grths, (64, 64), (32, 32)).detach().cpu()\n\n#         l2_distances = compute_l2_distance_per_patch_pair(patch_inputs, patch_grths)\n# #         l2_dis = torch.diagonal(l2_distances, dim1=1, dim2=2)\n        \n#         indices = torch.topk(l2_distances, dim=1, k=5, largest=False).indices\n        \n#         topk_output_patches = patch_outputs[torch.arange(patch_outputs.size(0)).unsqueeze(1), indices]\n#         topk_input_patches = patch_inputs[torch.arange(patch_inputs.size(0)).unsqueeze(1), indices]\n#         topk_grth_patches = patch_grths[torch.arange(patch_grths.size(0)).unsqueeze(1), indices]\n#         topk_output_patches = topk_output_patches.to(device)\n#         topk_input_patches = topk_input_patches.to(device)\n#         topk_grth_patches = topk_grth_patches.to(device)\n#         pil_img1 = topil(topk_input_patches[0][1])\n#         pil_img2 = topil(topk_grth_patches[0][1])\n#         print(\"k\")\n#         plt.imshow(pil_img1)\n#         plt.show()\n#         print(\"k\")\n#         plt.imshow(pil_img2)\n#         plt.show()\n#         break\n        \n        l_con = mse_loss(re_rain_img, input_images)\n#         w_l_con = w_l1_loss(topk_output_patches, topk_grth_patches)\n        l_rain = cl_neg(inp_rain_fea, inp_bg_fea) + 0.01*cl_neg(weakly_bg_fea, weakly_rain_fea)\n        l_bg = cl_pos(inp_bg_fea, weakly_bg_fea)# + cl_neg(weakly_bg_fea, weakly_rain_fea).mean()\n        l_sparsity = sparsity_loss(input_images, bg_img)\n\n        loss = l_rain + l_bg + l_con# + 0.0*l_sparsity# + w_l_con\n        training_loss += loss.item()\n        rain_loss += l_rain.item()\n        background_loss += l_bg.item()\n        con_loss += l_con.item()\n#         w_con_loss += w_l_con.item()\n        spar_loss += l_sparsity.item()\n#         print(f'sparsity loss: {spar_loss}, weak l1 loss: {w_con_loss}, rain loss: {rain_loss}, back loss: {background_loss}, mse loss: {con_loss}, sum loss: {training_loss}')\n\n\n#         print(f'sparsity loss: {spar_loss}, rain loss: {rain_loss}, back loss: {background_loss}, l1 loss: {con_loss}, weak l1 loss: {w_con_loss}, sum loss: {training_loss}')\n\n\n        loss.backward()\n        optimizer.step()\n#     break\n\n    print(f'sparsity loss: {spar_loss}, weak l1 loss: {w_con_loss}, rain loss: {rain_loss}, back loss: {background_loss}, mse loss: {con_loss}, sum loss: {training_loss}')\n    des = f'/kaggle/working/derain2024_{epoch}.pth'\n    torch.save(model.state_dict(), des)    ","metadata":{"execution":{"iopub.status.busy":"2024-05-23T16:54:11.859301Z","iopub.execute_input":"2024-05-23T16:54:11.859633Z"},"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}